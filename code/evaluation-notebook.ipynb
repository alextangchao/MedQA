{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07279598-5cb1-46f1-b710-e4bfba532908",
   "metadata": {},
   "source": [
    "# Evaluation-notebook \n",
    "\n",
    "I'm assuming you git pull the last changes from the repo to open this file.\n",
    "\n",
    "## Instructions\n",
    "First, add your OPENAI_API_KEY \n",
    "\n",
    "Select `Run all cells`, and it will:\n",
    "- install dependencies\n",
    "- generate the A/B_test output file.\n",
    "\n",
    "**You can send it to me via email or push it to github)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ad21f8b2-fbbe-433c-8e4d-e1075d5dd9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas together openai instructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8eecdf9c-92db-4543-b3eb-fcd3517f0942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from evaluate import load\n",
    "import pandas as pd \n",
    "import os\n",
    "import json\n",
    "import instructor\n",
    "from pydantic import BaseModel, Field\n",
    "from together import Together\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbb5497-871f-4da5-8fff-9a4ea2bc10c9",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a45adfd1-9100-4bf0-8073-85066318e4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results from medpalm\n",
    "medpalm = pd.read_csv(\"../output/med-palm-paragraph.csv\")\n",
    "questions = list(medpalm['question'])\n",
    "mp2_predictions = list(medpalm['predict'])\n",
    "references = list(medpalm['answer'])\n",
    "\n",
    "# load results from llama3\n",
    "llama3 = pd.read_csv(\"../output/llama3.csv\")\n",
    "# questions = list(llama3['question'])\n",
    "ll3_predictions = list(llama3['qlora-1844'])\n",
    "# references = list(llama3['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2f0dce-2510-4ea7-a2d9-7c271f56f5cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b49fa39a-7222-4c7b-a960-2395eb72212c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medpalm-2 \n",
      "================= \n",
      "PCOS is linked to changes in hormone levels that make it harder for the ovaries to release fully-grown (mature) eggs. The reasons for these changes are unclear. The hormones affected are:  - Estrogen and progesterone, the female hormones that help a woman's ovaries release eggs   - Androgen, a male hormone that is found in small amounts in women   Normally, one or more eggs are released during a woman's cycle. This is known as ovulation. In most cases, this release of eggs occurs about 2 weeks after the start of a menstrual period.   In PCOS, mature eggs are not released. Instead, they stay in the ovaries with a small amount of fluid around them. There can be many of these. However, not all women with the condition will have ovaries with this appearance.   These problems with the release of eggs can contribute to infertility. The other symptoms of this disorder are due to thehormoneimbalances.  Most of the time, PCOS is diagnosed in women in their 20s or 30s. However, it may also affect teenage girls. The symptoms often begin when a girl's periods start. Women with this disorder often have a mother or sister who has similar symptoms.) \n",
      "\n",
      "Llama qlora-1844 \n",
      "================= \n",
      "The cause of polycystic ovary syndrome (PCOS) is unknown. Scientists believe that several factors interact to cause PCOS, including insulin resistance, high androgens, and obesity. Insulin resistance Insulin is a hormone produced by the pancreas that helps cells throughout the body use glucose for energy. Insulin resistance means that the body's cells are less sensitive to the effects of insulin. Insulin resistance can lead to high blood sugar levels. High androgens Androgens are hormones that occur naturally in both women and men. Androgens are important for normal sexual development. In women, androgens are produced by the ovaries, the adrenal glands, and the placenta. In men, androgens are produced by the testicles. Normally, women produce small amounts of androgens. In women with PCOS, the ovaries produce high levels of androgens. Obesity Obesity is a medical condition in which a person has an abnormally high amount of body fat. Obesity increases the amount of hormones in the blood that can cause insulin resistance. Researchers believe that obesity may cause PCOS in women who are insulin resistant. \n",
      "\n",
      "llama-3 \n",
      "================= \n",
      "Polycystic ovary syndrome (PCOS) is a common health problem related to the female reproductive system. It is caused by an imbalance of hormones. This imbalance can cause a woman's body to make extra male hormones. These hormones are called androgens. Women with PCOS have higher than normal male hormone levels. This hormone imbalance can cause problems with a woman's menstrual cycle, fertility, and appearance.\n"
     ]
    }
   ],
   "source": [
    "num_row = 1\n",
    "print(f\"Medpalm-2 \\n================= \\n{medpalm.loc[num_row]['answer']} \\\n",
    "\\n\\nLlama qlora-1844 \\n================= \\n{llama3.loc[num_row]['qlora-1844']} \\\n",
    "\\n\\nllama-3 \\n================= \\n{llama3.loc[num_row]['llama3']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c808ea-3874-4576-90f1-7d7c3d64c313",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Bertscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e99bca4d-0230-4390-ac5e-ff2853ff61b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate BERTscore\n",
    "\n",
    "# bertscore = load(\"bertscore\")\n",
    "\n",
    "# bertscore = bertscore.compute(predictions=predictions, references=references, lang=\"en\") # precision, recall, f1score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd345662-1d8c-44f5-8ca6-b4ec11660d21",
   "metadata": {},
   "source": [
    "# Prompts for evals "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114c0d46-e2b6-4dff-8383-aa37dda47603",
   "metadata": {},
   "source": [
    "## Single answer eval (e.g  model_A or model_B answer against gold answer) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4879c871-93a3-4c59-9211-6ee2aa32d2c0",
   "metadata": {},
   "source": [
    "### Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9eb5a1af-c5b4-4331-9a32-2dee22eb053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## prompt for llm evaluation\n",
    "\n",
    "prompt = \\\n",
    "\"\"\"\n",
    "This is an evaluation task for a medical Question-Answering system, your task is to evaluate the predicted answer of medical question and compare it with the gold answer provided as a reference. \n",
    "\n",
    "The evaluation of the predicted answer would be based on four metrics: coherence, naturalness, correctness and completeness, you are asked to provide four ratings based on these metrics separately.\n",
    "\n",
    "Here are some instructions on rating the four metrics:\n",
    "1. **Correctness**: correctness addresses the faithfulness of an explanation with respect to the predictive model, this reflects whether the model understands the given question. You need to check that whether the prediction contains correct information about the question and if it is related to the question;\n",
    "2. **Coherence**: coherence describes how accordant the explanation is with prior knowledge and beliefs, you need to check the logical flow, thematic consistency, and stylistic uniformity in the predicted answer. This step is to evaluate the consistency of the output answer;\n",
    "3. **Naturalness**: naturalness evaluates how closely the predicted answer resembles a native human speaker. A natural answer should be fluent and indistinguishable from human written text, you need to check the similarity between the prediction and a human written text;\n",
    "4. **Completeness**: completeness measures how complete the prediction is. A complete answer should be comprehensive and thorough, covers all relevant aspects in the given question. You should check whether the prediction answers all aspects mentioned in the question compared to the gold answer and the question itself;\n",
    "\n",
    "Based on the instructions, you are required to output the rating based on a 10-point scale. 1 (incompatible to the instructions and incorrect), 5 (relates to the instructions but have some flaws) and 10 (perfectly aligns with the question and gold answer as well as the rating instructions ).\n",
    "\n",
    "The gold Q-A pairs and model prediction are:\n",
    "\n",
    "**Question**: {%s}\n",
    "**Gold answer**: {%s}\n",
    "**Prediction**: {%s}\n",
    "\n",
    "The output should be provided in JSON format as follows with no extra words and only the JSON dictionary:\n",
    "{\"question\": the input question, \"prediction\": given predicted answer, \"gold answer\": given gold answer, \"correctness\": the score for correctness, \"coherence\": score for coherence, \"naturalness\": score for naturalness, \"completeness\": score for completeness, \"reason\": reason for giving the ratings for the four scores}\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "TOGETHER_API_KEY = \"3f06bb7945192b6e58e9d4acd1d265fe51a1478015fb5cac5d67c3ca274b3c94\"\n",
    "base_url = \"https://api.together.xyz/v1\"\n",
    "model = \"META-LLAMA/LLAMA-3-70B-CHAT-HF\"\n",
    "#### llama3 70b\n",
    "\n",
    "client = Together(api_key = TOGETHER_API_KEY)\n",
    "\n",
    "#### GPT-4\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key = TOGETHER_API_KEY,\n",
    "    base_url = base_url\n",
    ")\n",
    "\n",
    "#### get evaluation\n",
    "\n",
    "Report_Single_Answer_Eval = pd.DataFrame()\n",
    "\n",
    "for i in range(len(questions)):\n",
    "    break\n",
    "    content = prompt% tuple([questions[i], references[i], ll3_predictions[i]])\n",
    "    response = client.chat.completions.create(\n",
    "    model=model, # llama3 70b: \"META-LLAMA/LLAMA-3-70B-CHAT-HF\"; GPT4: \"gpt-4\"\n",
    "    messages=[{\"role\": \"user\", \"content\": content}],\n",
    "    temperature = 0.2,\n",
    ")\n",
    "    report = response.choices[0].message.content\n",
    "    report = ast.literal_eval(report)\n",
    "    Report_Single_Answer_Eval = pd.concat([Report_Single_Answer_Eval, pd.DataFrame([report])], ignore_index=True)\n",
    "    break\n",
    "    if (i+1)%10 == 0:\n",
    "        print(\"-----------------------eval complete for \", i+1, \"predictions---------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d168416a-059b-4bb3-a246-82ccd85f6a84",
   "metadata": {},
   "source": [
    "### Results - Single answer eval (llama-3 against gold answer) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b8bfb5-b906-4511-8cae-9f121a228575",
   "metadata": {},
   "source": [
    "```\n",
    "question        What is (are) Polycystic ovary syndrome ? (Als...\n",
    "prediction      Polycystic ovary syndrome (PCOS) is a conditio...\n",
    "gold answer     Polycystic ovary syndrome is a condition in wh...\n",
    "correctness                                                    10\n",
    "coherence                                                      10\n",
    "naturalness                                                    10\n",
    "completeness                                                   10\n",
    "reason          The predicted answer is highly accurate and co...\n",
    "Name: 0, dtype: object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e0c72c-0b5a-4ab9-b878-03ac6bf34df8",
   "metadata": {},
   "source": [
    "## Pairs of answers eval (e.g  model_A and model_B answer against gold answer) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904a00a8-f73a-4280-8681-2a780be24578",
   "metadata": {},
   "source": [
    "### Together.ai json mode  || json output parser with pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "092ee4eb-ae77-4810-bd28-0c22f4937d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output parser\n",
    "class ModelResuts(BaseModel):\n",
    "    model : str = Field(description=\"given model name\")\n",
    "    correctness : str = Field(description=\"the score for correctness\")\n",
    "    coherence : str = Field(description=\"score for coherence\")\n",
    "    naturalness : str = Field(description=\"score for naturalness\")\n",
    "    completeness : str = Field(description=\"score for completeness\")\n",
    "    reason : str = Field(description=\"reason for giving the ratings for the four scores\")\n",
    "    \n",
    "class Report_Pair_eval(BaseModel):\n",
    "    question : str = Field(description=\"The input question\")\n",
    "    winner : str = Field(description=\"name of the model most similar to the gold answer, model_a or model_b\")\n",
    "    winner_reason : str = Field(description=\"reasons for selecting a model as a winner. Provide some details and concrete examples\")\n",
    "    modelA : ModelResuts\n",
    "    modelB : ModelResuts \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f4a294-ca7c-4fc4-b7b6-4b9c503dd2c7",
   "metadata": {},
   "source": [
    "### Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5e7a35a0-5194-4e8a-83d9-57184675ddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_template = \"\"\"\n",
    "    Evaluate the responses of two models, Llama-3 and MedPal-2, to a medical question against a gold standard answer. Rate each model response based on four metrics: correctness, coherence, naturalness, and completeness.\n",
    "\n",
    "    Here are some instructions on rating the four metrics:\n",
    "    1. **Correctness**: correctness addresses the faithfulness of an explanation with respect to the predictive model, this reflects whether the model understands the given question. You need to check that whether the prediction contains correct information about the question and if it is related to the question;\n",
    "    2. **Coherence**: coherence describes how accordant the explanation is with prior knowledge and beliefs, you need to check the logical flow, thematic consistency, and stylistic uniformity in the predicted answer. This step is to evaluate the consistency of the output answer;\n",
    "    3. **Naturalness**: naturalness evaluates how closely the predicted answer resembles a native human speaker. A natural answer should be fluent and indistinguishable from human written text, you need to check the similarity between the prediction and a human written text;\n",
    "    4. **Completeness**: completeness measures how complete the prediction is. A complete answer should be comprehensive and thorough, covers all relevant aspects in the given question. You should check whether the prediction answers all aspects mentioned in the question compared to the gold answer and the question itself;\n",
    "\n",
    "    Ratings should be on a 10-point scale:\n",
    "    - 1: Not aligned with instructions and incorrect.\n",
    "    - 5: Partially aligned with some flaws.\n",
    "    - 10: Fully aligned and perfectly accurate.\n",
    "\n",
    "    ========== Question =========\n",
    "    **Question**: {question}\n",
    "\n",
    "    ========== Gold Answer =========\n",
    "    **Gold Answer**: {gold_answer}\n",
    "\n",
    "    ========== Model A Answer =========\n",
    "    **Llama-3**: {model_a_answer}\n",
    "    \n",
    "    ========== Model B Answer =========\n",
    "    **MedPalm-2**: {model_b_answer}\n",
    "\n",
    "    Prefer answers that closely match the gold answer, even if it is non-responsive.\n",
    "\"\"\"\n",
    "additional_instructions = \"\"\"\n",
    "    Use the full rating range and focus on the similarity to the gold answer. Output the ratings in JSON format without additional text:\n",
    "    ========== Json example =========\n",
    "    {\n",
    "        \"question\": the question,\n",
    "        \"model_a\": {\n",
    "            \"model_name\": Llama-3,\n",
    "            \"correctness\": score,\n",
    "            \"coherence\": score,\n",
    "            \"naturalness\": score,\n",
    "            \"completeness\": score,\n",
    "            \"reason\": explanation\n",
    "        },\n",
    "        \"model_b\": {\n",
    "            \"model_name\": MedPalm-2,\n",
    "            \"correctness\": score,\n",
    "            \"coherence\": score,\n",
    "            \"naturalness\": score,\n",
    "            \"completeness\": score,\n",
    "            \"reason\": explanation\n",
    "        },\n",
    "        \"winner\": winner model name,\n",
    "        \"winner_reason\": explanation of why one model is better than the other\n",
    "    }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e801b129-23de-43bd-b5ae-e12fad647df4",
   "metadata": {},
   "source": [
    "### SET HERE YOU OPEN_AI_API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "581ecf07-68ec-4c2d-be9c-bd4dd33b4b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# first 500 evals\n",
    "num_ele = 2\n",
    "questions = questions[0:num_ele]\n",
    "references = references[0:num_ele]\n",
    "mp2_predictions = mp2_predictions[0:num_ele]\n",
    "ll3_predictions = ll3_predictions[0:num_ele]\n",
    "\n",
    "# Chaolong\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY_HERE\"\n",
    "# model = \"gpt-3.5-turbo\"\n",
    "# model = \"gpt-4\"\n",
    "model = \"gpt-3.5-turbo-0125\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b6c2f816-d55a-4976-8df4-716dc5a5e8bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.77s/it]\n"
     ]
    }
   ],
   "source": [
    "#### GPT-4\n",
    "client = instructor.from_openai(OpenAI())\n",
    "\n",
    "#### get evaluation\n",
    "report_pair_eval_df = pd.DataFrame()\n",
    "report = pd.DataFrame()\n",
    "for i in tqdm(range(len(questions))):\n",
    "    content = question_template.format(\n",
    "        question=questions[i],\n",
    "        gold_answer=references[i],\n",
    "        model_a_answer=ll3_predictions[i],\n",
    "        model_b_answer=mp2_predictions[i]) \n",
    "    # content += additional_instructions \n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model, \n",
    "        response_model = Report_Pair_eval,\n",
    "        messages=[{\"role\": \"user\", \"content\": content}],\n",
    "        temperature = 0,\n",
    "    )\n",
    "    \n",
    "    report_pair_eval_df = pd.concat([report_pair_eval_df, pd.DataFrame([response.dict()])], ignore_index=True)\n",
    "    if (i+1)%10 == 0:\n",
    "        print(\"-----------------------eval complete for \", i+1, \"predictions---------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84577d95-ad60-45ec-bb16-dd334c2b6a73",
   "metadata": {},
   "source": [
    "### Results (Llama-3 and MedPalm-2 against gold answer simultaneously) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f2cb36b8-60df-456c-91c5-61866d0270e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'Llama-3',\n",
       " 'correctness': '8',\n",
       " 'coherence': '7',\n",
       " 'naturalness': '6',\n",
       " 'completeness': '8',\n",
       " 'reason': 'Llama-3 accurately identifies insulin resistance, high androgens, and obesity as factors contributing to PCOS, aligning well with the gold answer. The response is coherent and mostly complete, but could be more natural in its language.'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_pair_eval_df.loc[1][\"modelA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0a2c398e-68c0-4be2-9f28-e8b1e8baef85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'MedPalm-2',\n",
       " 'correctness': '7',\n",
       " 'coherence': '8',\n",
       " 'naturalness': '8',\n",
       " 'completeness': '7',\n",
       " 'reason': 'MedPalm-2 provides a comprehensive overview of PCOS, mentioning genetic, environmental, and lifestyle factors. While the response is coherent and natural, it lacks some specific details mentioned in the gold answer, such as the impact of high androgens and obesity on PCOS.'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_pair_eval_df.loc[1][\"modelB\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f70f11e2-9c1f-4062-b255-eac05786b86c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama-3'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_pair_eval_df.loc[1][\"winner\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fa26fdf8-eac5-4ab7-b9e3-70ab11a5103c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama-3 provides a more detailed and accurate explanation of the causes of Polycystic ovary syndrome, mentioning insulin resistance, high androgens, and obesity as contributing factors. The response is aligned with the gold answer in terms of hormone imbalances and the impact on insulin resistance and androgens.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_pair_eval_df.loc[1][\"winner_reason\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2267a005-ad96-461f-a59c-f5bcef9491bd",
   "metadata": {},
   "source": [
    "### Save results as .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a1f3946a-4e2c-4f14-8485-8abbc063887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_file = f\"../output/ab_test_{model}_{num_ele}_pairs.csv\"\n",
    "report_pair_eval_df.to_csv(path_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8c68d9ee-286d-4386-9151-30ff5f12f5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Push the output file created or send it to me via email\n"
     ]
    }
   ],
   "source": [
    "ab_test = pd.read_csv(path_file)\n",
    "ab_test\n",
    "print(\"Push the output file created or send it to me via email\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef32ed0-0ea7-4be5-9564-fbd8c8759b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.10 (685_final_project_reproducibility2)",
   "language": "python",
   "name": "685_final_project_env_reproduciability2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
